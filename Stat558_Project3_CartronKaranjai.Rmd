---
title: "Stat558_Project3_CartronKaranjai"
author: "Matthieu Cartron and Sneha Karanjai"
date: "11/11/2022"
output: html_document
---

# Setup

Note: to hide later if necessary.

## Libraries

```{r lib, message=FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(caret)
library(brainGraph)
library(corrplot)
library(GGally)
```

# Introduction

The following analysis uses the "Online News Popularity" data set from the UCI machine learning repository. It consists of a number variables describing different features of articles, each of which belonging to one of six "channels." These channels are effectively genres, and are the following: 

- Lifestyle
- Entertainment
- Business
- Social Media
- World News

For this analysis, we are primarily concerned with the "shares" variable, which simply describes the number of times an article has been shared. We often hear that news travels more quickly depending on its content, title, and maybe even the number of images it uses. In a similar vein, we would like, for each of the different data channels, to use certain variables describing the articles to predict the number of times an article might be shared. But how do we know which variables to choose?

We could use simple intuition to pick variables. For example, it makes sense to think that articles with high sentiment polarity (positive or negative) would tend to, on average, have more shares. We could go through the variables and pick those that we think would have the greatest impact on the number of shares. The issue, however, is that they may change from one data channel to the next. Are lifestyle articles and world news articles going to be affected by the same variables? If we choose the same variables across all the different data channels, then this is the assumption we will be making. To avoid making this assumption, we will automate the process of variable selection by deleting one variable of each of the pairs of colinear variables.



# Data

```{r readingdata, message=FALSE, warning=FALSE}
unzippedNewDataCSV <- unzip("OnlineNewsPopularity.zip")

newsDataName <- read_csv(unzippedNewDataCSV[1]) # This is the names file
newsData <- read_csv(unzippedNewDataCSV[2])

head(newsData)
```

```{r}
newsData %>% 
  select(starts_with("data_channel_is_"))
```
Again, the 6 data channels to analyze are : 

- Lifestyle 
- Entertainment 
- Business
- Social Media 
- Technology 
- World 

We will filter the data to analyze articles in one data channel at a time. Additionally according to the data report, `url` and `timedelta` are two non-predictive columns so we will remove them.

```{r subsetdata}
subsettingData <- function(data, area){
  #getting the naming convention as per the dataframe
  key <- ifelse(tolower(area)=="lifestyle", "lifestyle",
                ifelse(tolower(area)=="entertainment", "entertainment",
                       ifelse(tolower(area)=="business", "bus",
                              ifelse(tolower(area)=="social media", "socmed",
                                     ifelse(tolower(area)=="technology", "tech",
                                            ifelse(tolower(area)=="world", "world", "NA"))))))
  subsetVar <- paste("data_channel_is_", key, sep = "")
  
  # filtering the data and removing the data_channel_is_ columns, url, and timedelta
  subsetData <- data %>% 
    filter(!!as.symbol(subsetVar)==1) %>% 
    select(-c(starts_with("data_channel_is_"), url, timedelta))
  
  return(list(subsetData, subsetVar))
}

subsettingDataReturn <- subsettingData(newsData, "lifestyle")
data <- subsettingDataReturn[[1]]
channel <- subsettingDataReturn[[2]]
```

# Exploratory Data Analysis 

Let us take a look at the columns available. 

```{r}
colnames(data)
```

Whew! That is a long list of columns to analyze. Instead of analyzing them all, let us think about our data. What might we expect to be related to how many times an article is shared? We hear frequently about how the dissemination of news and the content thereof are related in some way. For our data channels, let's pay close attention to the number of shares (dissemination) and variables that we might be able to link to it. Can we find any interesting relationships in the exploratory analysis? And do these relationships change across the different channels? Maybe the sharing of lifestyle articles is less correlated with sentiment than, say, world news articles.  


First, let's take a look at the variable descriptions for some better understanding. Here is a data description from the UCI Machine Learning Repository: 

- n_tokens_title: Number of words in the title
- n_tokens_content Number of words in the content
- n_unique_tokens: Rate of unique words in the content
- n_non_stop_unique_tokens: Rate of unique non-stop words in the content
- num_hrefs: Number of links
- num_self_hrefs: Number of links to other articles published by Mashable
- num_imgs: Number of images
- num_videos: Number of videos
- average_token_length: Average length of the words in the content
- num_keywords: Number of keywords in the metadata
- self_reference_min_shares: Min. shares of referenced articles in Mashable
- self_reference_max_shares: Max. shares of referenced articles in Mashable
- self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
- global_subjectivity: Text subjectivity
- global_sentiment_polarity: Text sentiment polarity
- global_rate_positive_words: Rate of positive words in the content
- global_rate_negative_words: Rate of negative words in the content
- rate_positive_words: Rate of positive words among non-neutral tokens
- rate_negative_words: Rate of negative words among non-neutral tokens
- title_subjectivity: Title subjectivity
- title_sentiment_polarity: Title polarity
- abs_title_subjectivity: Absolute subjectivity level
- abs_title_sentiment_polarity: Absolute polarity level
- shares: Number of shares (target)


Below we run the five-number summary for each of the variables thus far still included. 

```{r summarystats}
print(paste("******Summary Statistics of", channel, "******"))
summary(data)
```

Feeding all these variables into the training models would mean "Garbage In and Garbage Out". One of the easiest ways to choose the variables to fit into the models is by checking the correlation. Potential predictors with high correlation between each other can prove problematic as they introduce multicollinearity into the model. We can remove some of this redundancy from the outset. 

Let us first understand the pair plots for all the variables explaining keywords.

```{r}
pairs(~ kw_min_min + kw_max_min + kw_min_max + kw_avg_max + kw_max_avg + kw_avg_min + kw_max_max + kw_min_avg + kw_avg_avg, data = data)
```


```{r}
cor(data[, c('kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_max_avg', 'kw_avg_min', 'kw_max_max', 'kw_min_avg', 'kw_avg_avg')])
```


```{r}
kwCorData <- as.data.frame(as.table(cor(data[, c('kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_max_avg', 'kw_avg_min', 'kw_max_max', 'kw_min_avg', 'kw_avg_avg')])))

colRemove <- kwCorData %>% 
  filter(abs(Freq)>0.8 & Freq!=1 )

colRemove <- as.vector(colRemove$Var2)

data <- data %>% 
  select(-all_of(colRemove))
```

This removes all the highly correlated keyword variables that convey the same information. Now we will similarly investigate the self-referenced shares. 


```{r}
pairs(~ self_reference_avg_sharess + self_reference_max_shares + self_reference_min_shares, data = data)
```

If we find that any of the self_reference shares variables have a correlation of greater than 0.8 with one another, then we will eliminate it from the analysis. Again, this is done to limit the multicollinearity in the models we build below as well as reduce their dimension. We want to simplify our models from the outset as much as possible without losing predictors that will explain much of the variability in the number of times an article is shared. 

```{r}
srCorData <- as.data.frame(as.table(cor(data[, c('self_reference_avg_sharess', 'self_reference_max_shares', 'self_reference_min_shares')])))

colRemove <- srCorData %>% 
  filter(abs(Freq)>0.8 & Freq!=1 )

colRemove <- as.vector(colRemove$Var2)

data <- data %>% 
  select(-all_of(colRemove))
```

In this next step, we examine our remaining variables to see if any share a correlation of 0.8 or higher. If so, we will remove it from the data. 

```{r}
cols <- names(data)
corrDf <- data.frame(t(combn(cols,2)), stringsAsFactors = F) %>%
  rowwise() %>%
  mutate(v = cor(data[,X1], data[,X2]))

corrDf <- corrDf %>% 
  filter(abs(v)>0.8)
corrDf
```
We will remove the columns in X2 from our analysis. Again, we do not want to remove both. For example, if we were looking at the variables temperature in Farenheit and temperature in Celcius in predicting the number of people at a beach, both variable would be telling us the same thing, but we would still want to keep one of them because of its probable importance to the model. 

```{r}
data <- data %>% 
  select(-c(corrDf$X2))
data
```
We will also remove `is_weekend` from our analysis as the variables `weekday_is_sunday` and `weekday_is_saturday` capture the same information. 

```{r}
data <- data %>% 
  select(-c("is_weekend"))
```




We are now down from `r ncol(newsData)` columns to `r ncol(data)` columns.

Let us finally do a correlation plot for all variables with threshold greater than 0.55 for the the present dataframe. 

```{r}
cols <- names(data)

corrDf <- data.frame(t(combn(cols,2)), stringsAsFactors = F) %>%
  rowwise() %>%
  mutate(v = cor(data[,X1], data[,X2]))

corrDf <- corrDf %>% 
  filter(abs(v)>0.55) %>% 
  arrange(desc(v))
corrDf

#turn corr back into matrix in order to plot with corrplot
correlationMat <- reshape2::acast(corrDf, X1~X2, value.var="v")
  
#plot correlations visually
corrplot(correlationMat, is.corr=FALSE, tl.col="black", na.label=" ")
```

Now that we have a more concise data set, let's zero in on the relationship between our target variable, shares, and the remaining variables. Below we extract the five variables that have the highest correlation with the shares variable. This may be a valuable insight prior to training our models.  

```{r}
sharesCor <- cor(data[ , colnames(data) != "shares"],  # Calculate correlations
                data$shares)

sharesCor <- data.frame(sharesCor)
sharesCor$names <- rownames(sharesCor)
rownames(sharesCor) <- NULL

sharesCor <- sharesCor %>% 
  rename(corrcoeff=sharesCor) %>% 
  arrange(desc(abs(corrcoeff))) %>% 
  head(6)

par(mfrow=c(2,3))
for (i in sharesCor$names) {
  plot(data$shares, data[[i]], ylab = i, main = paste("Shares vs", i))
}


#for (i in sharesCor$names) {
 # ggpairs(data[[i]], ylab = i, main = paste("Shares vs", i))
#}
```

Let's also take a look at the distribution of our target variable using a histogram.

```{r}
ggplot(data) +
  aes(x = shares) +
  geom_histogram(bins = 26L, fill = "#112446") +
  labs(title = "Distribution of Shares") +
  theme_gray()
```

Now let's analyze the affect of the different variables on the shares. Starting with the number of words in the title and how they affect the shares. 

```{r}
data %>% 
  group_by(n_tokens_title) %>% 
  summarise(avgShares = mean(shares)) %>% 
  ggplot() +
  aes(x = avgShares, y = n_tokens_title) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Average Shares vs Title Tokens") +
  theme_gray()
```

Expected Behavior : Readers prefer titles with fewer words. 

```{r}
data %>% 
  group_by(factor(num_imgs)) %>% 
  summarise(sumShares = sum(shares)) %>% 
  ggplot() +
  aes(x = `factor(num_imgs)`, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Shares vs Images", x = "Number of Images", y = "Shares(Sum)") +
  theme_minimal()
```

Expected Behavior : Fewer images do better. 

```{r}
data %>% 
  group_by(factor(num_videos)) %>% 
  summarise(sumShares = sum(shares)) %>% 
  ggplot() +
  aes(x = `factor(num_videos)`, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Shares vs Videos", x = "Number of Videos", y = "Shares(Sum)") +
  theme_minimal()
```

Expected Behavior: 0-1 Videos do the best. 

```{r}
mon <- data %>% 
  select(starts_with("weekday_is_monday"), shares) %>% 
  group_by(weekday_is_monday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_monday) %>% 
  filter(day==1) 
mon$day[mon$day==1] <- "MON"

tue <- data %>% 
  select(starts_with("weekday_is_tuesday"), shares) %>% 
  group_by(weekday_is_tuesday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_tuesday) %>% 
  filter(day==1)
tue$day[tue$day==1] <- "TUE"


wed <- data %>% 
  select(starts_with("weekday_is_wednesday"), shares) %>% 
  group_by(weekday_is_wednesday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_wednesday) %>% 
  filter(day==1)
wed$day[wed$day==1] <- "WED"


thu <- data %>% 
  select(starts_with("weekday_is_thursday"), shares) %>% 
  group_by(weekday_is_thursday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_thursday) %>% 
  filter(day==1)
thu$day[thu$day==1] <- "THU"

fri <- data %>% 
  select(starts_with("weekday_is_friday"), shares) %>% 
  group_by(weekday_is_friday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_friday) %>% 
  filter(day==1)
fri$day[fri$day==1] <- "FRI"

sat <- data %>% 
  select(starts_with("weekday_is_saturday"), shares) %>% 
  group_by(weekday_is_saturday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_saturday) %>% 
  filter(day==1)
sat$day[sat$day==1] <- "SAT"

sun <- data %>% 
  select(starts_with("weekday_is_sunday"), shares) %>% 
  group_by(weekday_is_sunday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_sunday) %>% 
  filter(day==1)
sun$day[sun$day==1] <- "SUN"

mon %>% 
  bind_rows(tue, wed, thu, fri, sat, sun) %>% 
  ggplot() +
  aes(x = day, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Most Shared Articles by the Day of the Week") +
  theme_gray()
```

Expected Behavior : Greatest number of articles is shared over weekends.

Polarity is a float which lies in the range of [-1,1] where 1 refers to a positive statement and -1 refers to a negative statement. Does title polarity affect the average number of shares?

```{r}
 
  
```

How does text polarity affect shares?

```{r}

```

Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is a float which lies in the range of [0,1]. A value closer to 0 means an opinion or an emotion and 1 means a fact. How does the text having a factual tone or an author's emotion/opinion affect the total shares?

```{r}

```

# Data Preparation

Data splitting is an important aspect of data science, particularly for creating predictive models based on data. This technique helps ensure the creation of data models and processes that use data models -- such as machine learning -- are accurate. In a basic two-part data split, the training data set is used to train and fit models. Training sets are commonly used to estimate different parameters or to compare different model performance. The testing data set is used after the training is done; we see if our trained models are effective in predicting future values. We will use a 70-30 split on the dataset.

```{r}
train_index <- createDataPartition(data$shares, p = 0.7, 
                                   list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]
```

We will check the shape of the train and test set

```{r}
print("The train set dimensions")
dim(train)
print("The test set dimensions")
dim(test)
```

# Modeling

We will be comparing linear and ensemble techniques for predicting shares. Each section below elucidates the model used and the reasoning behind it. 

## Linear Regression 

A simple linear regression refers to a linear equation that captures the relationship between a response variable, $Y$, and a predictor variable $X$.  The relationship is modeled below:

$Y = \beta_0 + \beta_1X_1 +\epsilon i$

Where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line. This relationship can be extended to the case in which the response variable is modeled as a function of more than one predictor variable. This is the case of a multiple linear regression, which is as follows:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + … + \beta_nX_n + \epsilon i$

Where $\beta_0$ is the intercept and all $\beta$ are slope coefficients. For both simple and multiple linear regression cases, the Method of Least Squares is widely used in summarizing the data. The least squares method minimizes values of $\beta_0$ and all $\beta_n$, seen below:

$\sum_{i = 1}^{n} (yi - \beta_0 - \sum_{j = 1}^{k} \beta_j x_{ij}^2)$

Since we are dealing with `r ncol(data)` variables, it is probably important to know that we would need to employ a feature selection/dimension reduction technique. Feature selection is the process of reducing the number of input variables when developing a predictive model.
It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. To prove this, we will first fit a full-model (with all the available variables) with multiple linear regression. 

```{r, warning=FALSE}
trControl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

mlrWithoutVS <- train(shares ~ .,
                     data = train,
                     preProcess = c("center", "scale"),
                     method = "lm", trControl = trControl)

summary(mlrWithoutVS)
```

As we can see, the Root Mean Square Error of the model is `mlrWithoutVS$results$RMSE`. Now let us see how this changes with the Principle Components Analysis below.  

Principle Components Analysis (PCA) is a dimension-reduction technique that can be extended to regression. In a PCA we find linear combinations of the predictor variables that account for most of the variability in the model. What this does is it reduces the number of variables $p$ into $m$ principal components, allowing for a reduction in complexity all the while retaining most of the variability of the $p$ variables. We extend this to regression by treating our $m$ principle components as predictors, though we cannot interpret them in the same way.  


Let us check the model for multicollinearity. 

```{r}
pcs <- prcomp(train, scale = TRUE, center = TRUE)
summary(pcs)
```
How many principle components should we use? This is somewhat subjective. Consider the plot below. How many principle components would be required in order to retain say 80 or 90 percent of the variability in the data? If we can effectively reduce the number of variables by way of this method, then we may want to consider a regression of these principle components, even if we lose some interpretability. 

```{r}
par(mfrow = c(1, 2))
plot(pcs$sdev^2/sum(pcs$sdev^2), xlab = "Principal Component", 
		 ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(pcs$sdev^2/sum(pcs$sdev^2)), xlab = "Principal Component", 
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
```

```{r}
pcaVar <- as.vector(cumsum(pcs$sdev^2/sum(pcs$sdev^2)))
for (i in seq_along(pcaVar)) {
  if(pcaVar[i] > 0.9 & pcaVar[i] < 0.92){
    pcaIndex = i
  }
}

pc_train <- predict(pcs, train)
pc_train <- data.frame(pc_train)
pc_train <- pc_train %>% 
  select(c(1:pcaIndex)) %>% 
  mutate(shares = train$shares)
pc_test <- predict(pcs, test)
```

We will now fit a multiple linear regression using these principle components. 

```{r}
mlrWitVS <- train(shares ~ .,
                  data = pc_train,
                  preProcess = c("center", "scale"),
                  method = "lm", 
                  trControl = trControl)

summary(mlrWitVS)
```

Now that we have a Multiple Linear Regression with PCA, let us see how a Lasso Regression on the original data set competes in terms of variable selection. 

```{r, warning=FALSE}
tuneGrid <- expand.grid(
  .fraction = seq(0, 1, by = 0.1)
)

lassoModel <- train(
  shares ~ .,
  data = train,
  method = 'lasso',
  preProcess = c("center", "scale"),
  trControl = trControl,
  tuneGrid = tuneGrid
)

lassoFinalModel <- as_tibble(lassoModel$results) %>% 
  filter(fraction==lassoModel$bestTune$fraction) 
```


```{r}
lassoFinalModel
```

## Random Forest

The random forest model refers to an ensemble method of either classification or regression. In this case, we are predicting a continuous response variable, and are thus using the latter case. The random forest creates numerous trees from bootstrap samples of the data. Bootstrap samples are simply samples taken from the data and are of the same size (sample $n$ equals bootstrap $m$), meaning that an observation from the sample data could be used twice in the bootstrap sample, for example. A tree is fit to each bootstrap sample, and for each fit a random subset (generally $m = p/3$ predictors) of predictors is chosen. This is done with the tuning parameter mtry. 

Generally speaking, random forests predict more accurately because the results of the fitted trees are averaged across all trees. This averaging reduces variance. 

```{r}
tuneGrid = expand.grid(mtry = 1:10)

rfWithPC <- train(shares ~ .,
                  data = pc_train,
                  method = "rf", 
                  trControl = trControl,
                  tuneGrid = tuneGrid)

rfFinalModel <- as_tibble(rfWithPC$results) %>% 
  filter(mtry == rfWithPC$bestTune$mtry) 
```

```{r}
rfFinalModel
```

## Boosted Tree

Boosted tree models, like the random forest, are an ensemble tree-based method that can be used for classification or regression. Again, in our case, we are predicting a continuous response and are using regression. 

Unlike the random forest, in the boosted tree method, trees are grown sequentially, and for each tree the residuals are treated as the response. This is exactly true for the first tree. Updated predictions can be modeled by the following:

$\hat{y} = \hat{y}(x) + \lambda \hat{y}^b (x)$

Below we fit our boosted tree model to the training data set.

```{r, results=FALSE}
tuneGrid = expand.grid(n.trees = c(25, 50, 100, 150, 200), interaction.depth = c(1, 2, 3, 4), shrinkage = 0.1, n.minobsinnode = 10)

# fit the model
boostedWithPC <- train(shares ~ .,
                  data = pc_train,
                  method = "gbm", 
                  trControl = trControl,
                  tuneGrid = tuneGrid,
                  verbose = FALSE)

boostedFinalModel <- as_tibble(boostedWithPC$results) %>% 
  filter(n.trees==boostedWithPC$bestTune$n.trees,
         interaction.depth==boostedWithPC$bestTune$interaction.depth,
         shrinkage==boostedWithPC$bestTune$shrinkage,
         n.minobsinnode==boostedWithPC$bestTune$n.minobsinnode) 
```


# Model Comparisons

Now that we have created our trained models (models fit to the training data) we should now see how accurately they predict future values. Once we have evaluated each of the models, we should be able to compare them to see which is best at making predictions on future data. We can do this by comparing the predicted values of the tested with the actual test set values. 

## Linear Regression Evaluation

Withe function `postResample()` we can find our RMSE for our linear regression on the test data set. How does it compare with the other models? 

```{r}
PredLinearTest <- predict(mlrWithoutVS, test)

postResample(pred = PredLinearTest, obs = test$shares)
```

## PCA Regression Evaluation

```{r}
PredPCAtest <- predict(mlrWitVS, test)

postResample(pred = PredPCAtest, obs = test$shares)
```

## LASSO Regression

```{r}
#PredLassoTest <- predict(lassoModel$, test)

#postResample(pred = PredLassoTest, test$shares)
```


