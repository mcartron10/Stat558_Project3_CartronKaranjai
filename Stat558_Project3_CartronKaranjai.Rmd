---
title: "Stat558_Project3_CartronKaranjai"
author: "Matthieu Cartron and Sneha Karanjai"
date: "11/11/2022"
output: html_document
---

# Setup

Note: to hide later if necessary.

## Libraries

```{r lib, message=FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(caret)
library(brainGraph)
library(corrplot)
library(GGally)
```

# Introduction

# Data

```{r readingdata, message=FALSE, warning=FALSE}
unzippedNewDataCSV <- unzip("OnlineNewsPopularity.zip")

newsDataName <- read_csv(unzippedNewDataCSV[1]) # This is the names file
newsData <- read_csv(unzippedNewDataCSV[2])

head(newsData)
```

```{r}
newsData %>% 
  select(starts_with("data_channel_is_"))
```
The 6 groups to analyze are : 

- Lifestyle 
- Entertainment 
- Business
- Social Media 
- Technology 
- World 

We will filter the data to analyze articles in one data channel at a time. Additionally according to the data report, `url` and `timedelta` are two non-predictive columns so we will remove them.

```{r subsetdata}
subsettingData <- function(data, area){
  #getting the naming convention as per the dataframe
  key <- ifelse(tolower(area)=="lifestyle", "lifestyle",
                ifelse(tolower(area)=="entertainment", "entertainment",
                       ifelse(tolower(area)=="business", "bus",
                              ifelse(tolower(area)=="social media", "socmed",
                                     ifelse(tolower(area)=="technology", "tech",
                                            ifelse(tolower(area)=="world", "world", "NA"))))))
  subsetVar <- paste("data_channel_is_", key, sep = "")
  
  # filtering the data and removing the data_channel_is_ columns, url, and timedelta
  subsetData <- data %>% 
    filter(!!as.symbol(subsetVar)==1) %>% 
    select(-c(starts_with("data_channel_is_"), url, timedelta))
  
  return(list(subsetData, subsetVar))
}

subsettingDataReturn <- subsettingData(newsData, "lifestyle")
data <- subsettingDataReturn[[1]]
channel <- subsettingDataReturn[[2]]
```

# Exploratory Data Analysis 

Let us take a look at the columns available. 

```{r}
colnames(data)
```

<<<<<<< HEAD
Whew! That is a long list of columns to analyze. Instead of analyzing them all, let us think about our data. What might we expect to be related to how many times an article is shared? We hear frequently about how the dissemination of news and the content thereof are related in some way. For our data channels, let's pay close attention to the number of shares (dissemination) and variables that we might be able to link to it. Can we find any interesting relationships in the exploratory analysis? And do these relationships change across the different channels? Maybe the sharing of lifestyle articles is less correlated with sentiment than, say, world news articles.  




=======
First, let's take a look at the variable descriptions for some better understanding. Here is a data description from the UCI Machine Learning Repository: 

- n_tokens_title: Number of words in the title
- n_tokens_content Number of words in the content
- n_unique_tokens: Rate of unique words in the content
- n_non_stop_unique_tokens: Rate of unique non-stop words in the content
- num_hrefs: Number of links
- num_self_hrefs: Number of links to other articles published by Mashable
- num_imgs: Number of images
- num_videos: Number of videos
- average_token_length: Average length of the words in the content
- num_keywords: Number of keywords in the metadata
- self_reference_min_shares: Min. shares of referenced articles in Mashable
- self_reference_max_shares: Max. shares of referenced articles in Mashable
- self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
- global_subjectivity: Text subjectivity
- global_sentiment_polarity: Text sentiment polarity
- global_rate_positive_words: Rate of positive words in the content
- global_rate_negative_words: Rate of negative words in the content
- rate_positive_words: Rate of positive words among non-neutral tokens
- rate_negative_words: Rate of negative words among non-neutral tokens
- title_subjectivity: Title subjectivity
- title_sentiment_polarity: Title polarity
- abs_title_subjectivity: Absolute subjectivity level
- abs_title_sentiment_polarity: Absolute polarity level
- shares: Number of shares (target)



```{r summarystats}
print(paste("******Summary Statistics of", channel, "******"))
summary(data)
```

Feeding all these variables into the training models would mean "Garbage In and Garbage Out". One of the easiest ways to choose the variables to fit into the models is by checking the correlation. Potential predictors with high correlation between each other can prove problematic as they introduce collinearity into the model. We can remove some of this redundancy from the outset. 

```{r}
cols <- names(data)
corrDf <- data.frame(t(combn(cols,2)), stringsAsFactors = F) %>%
  rowwise() %>%
  mutate(v = cor(data[,X1], data[,X2]))

corrDf <- corrDf %>% 
  filter(abs(v)>0.8)
corrDf
```

We will remove the columns in X2 from our analysis.

```{r}
data1 <- data %>% 
  select(-c(corrDf$X2))
data1
```
We are now down from `r ncol(newsData)` columns to `r ncol(data)` columns. Let us finally do a correlation plot for all variables with threshold greater than 0.55 for the the present dataframe. 

```{r}
cols <- names(data1)

corrDf <- data.frame(t(combn(cols,2)), stringsAsFactors = F) %>%
  rowwise() %>%
  mutate(v = cor(data1[,X1], data1[,X2]))

corrDf <- corrDf %>% 
  filter(abs(v)>0.55)
corrDf

#turn corr back into matrix in order to plot with corrplot
correlationMat <- reshape2::acast(corrDf, X1~X2, value.var="v")
  
#plot correlations visually
corrplot(correlationMat, is.corr=FALSE, tl.col="black", na.label=" ")
```
Understanding the distribution of the target variable 


```{r}
ggplot(data1) +
  aes(x = shares) +
  geom_histogram(bins = 26L, fill = "#112446") +
  labs(title = "Distribution of Shares") +
  theme_gray()


```

Now let's analyze the effect of the different variables on the number of shares, starting with the number of words in the title and how they affect the shares. 

```{r}
data1 %>% 
  group_by(n_tokens_title) %>% 
  summarise(avgShares = mean(shares)) %>% 
  ggplot() +
  aes(x = avgShares, y = n_tokens_title) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Average Shares vs Title Tokens") +
  theme_gray()
```
>>>>>>> 9eb70331505dd712856296b2a7ec02a6ca4fbb20

## Automating Scatter Plots by Channel

Correlation with Shares: 

```{r}
sharescor <- cor(data[ , colnames(data) != "shares"], data$shares)

sharescor <- as.data.frame(sharescor)

top_five_shares_corr <- sharescor %>% 
  arrange(desc(abs(sharescor))) %>%
  slice(1:5) 
  
top_five_vars <- cbind(rownames(top_five_shares_corr), top_five_shares_corr)
rownames(top_five_vars) <- NULL

top_five_vars
```

Now create placeholder variables for the five variables with the highest correlation with shares.

```{r}
Top1 <- top_five_vars[1, 1]

df <- colnames(data) %in% Top1[1]


```

Note: What I would like to do: 

Match the variables with the highest absolute value of correlation with the actual columns. From there create scatter plots for each of those variables with the shares variable. I am a little stuck on how to code this for now. 


# Modeling


## Linear Regression

A simple linear regression refers to a linear equation that captures the relationship between a response variable, $Y$, and a predictor variable $X$.  The relationship is modeled below:

$Y = \beta_0 + \beta_1X_1 +\epsilon i$

Where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line. This relationship can be extended to the case in which the response variable is modeled as a function of more than one predictor variable. This is the case of a multiple linear regression, which is as follows:

$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + â€¦ + \beta_nX_n + \epsilon i$

Where $\beta_0$ is the intercept and all $\beta$ are slope coefficients. For both simple and multiple linear regression cases, the Method of Least Squares is widely used in summarizing the data. The least squares method minimizes values of $\beta_0$ and all $\beta_n$, seen below:

$\sum_{i = 1}^{n} (yi - \beta_0 - \sum_{j = 1}^{k} \beta_j x_{ij}^2)$



## Random Forest

The random forest model refers to an ensemble method of either classification or regression. In this case, we are predicting a continuous response variable, and are thus using the latter case. The random forest creates numerous trees from bootstrap samples of the data. Bootstrap samples are simply samples taken from the data and are of the same size (sample $n$ equals bootstrap $m$), meaning that an observation from the sample data could be used twice in the bootstrap sample, for example. A tree is fit to each bootstrap sample, and for each fit a random subset (generally $m = p/3$ predictors) of predictors is chosen. This is done with the tuning parameter mtry. 

Generally speaking, random forests predict more accurately because the results of the fitted trees are averaged across all trees. This averaging reduces variance. 


## Boosted Tree

Boosted tree models, like the random forest, are an ensemble tree-based method that can be used for classification or regression. Again, in our case, we are predicting a continuous response and are using regression. 

Unlike the random forest, in the boosted tree method, trees are grown sequentially, and for each tree the residuals are treated as the response. This is exactly true for the first tree. Updated predictions can be modeled by the following:

$\hat{y} = \hat{y}(x) + \lambda \hat{y}^b (x)$

## Model Comparison
