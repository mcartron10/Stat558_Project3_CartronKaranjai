---
title: "Online News Popularity Analysis"
author: "Matthieu Cartron and Sneha Karanjai"
date: "`r Sys.Date()`"
output: 
  github_document :
    toc : true 
    number_sections: true
params:
  channel : "lifestyle" 
---

# Introduction

The following analysis uses the "Online News Popularity" data set from the UCI machine learning repository. It consists of a number variables describing different features of articles, each of which belonging to one of six "channels." These channels are effectively genres, and are the following: 

- Lifestyle
- Entertainment
- Business
- Social Media
- World News

For this analysis, we are primarily concerned with the "shares" variable, which simply describes the number of times an article has been shared. We often hear that news travels more quickly depending on its content, title, and maybe even the number of images it uses. In a similar vein, we would like, for each of the different data channels, to use certain variables describing the articles to predict the number of times an article might be shared. But how do we know which variables to choose?

We could use simple intuition to pick variables. For example, it makes sense to think that articles with high sentiment polarity (positive or negative) would tend to, on average, have more shares. We could go through the variables and pick those that we think would have the greatest impact on the number of shares. The issue, however, is that they may change from one data channel to the next. Are lifestyle articles and world news articles going to be affected by the same variables? If we choose the same variables across all the different data channels, then this is the assumption we will be making. To avoid making this assumption, we will automate the process of variable selection by deleting one variable of each of the pairs of collinear variables.

# Setup

```{r lib, message=FALSE, warning=FALSE, error=FALSE}
library(tidyverse)
library(caret)
library(brainGraph)
library(corrplot)
library(GGally)
```

# Data Import 

```{r readingdata, message=FALSE, warning=FALSE}
unzippedNewDataCSV <- unzip("OnlineNewsPopularity.zip")

newsDataName <- read_csv(unzippedNewDataCSV[1]) # This is the names file
newsData <- read_csv(unzippedNewDataCSV[2])

head(newsData)
```
We will subset the data according to the channel passed to analyze articles in one data channel at a time. Additionally according to the data report, `url` and `timedelta` are two non-predictive columns so we will remove them.

```{r subsetdata}
subsettingData <- function(data, area){
  #getting the naming convention as per the dataframe
  key <- ifelse(tolower(area)=="lifestyle", "lifestyle",
                ifelse(tolower(area)=="entertainment", "entertainment",
                       ifelse(tolower(area)=="business", "bus",
                              ifelse(tolower(area)=="social media", "socmed",
                                     ifelse(tolower(area)=="technology", "tech",
                                            ifelse(tolower(area)=="world", "world", "NA"))))))
  subsetVar <- paste("data_channel_is_", key, sep = "")
  
  # filtering the data and removing the data_channel_is_ columns, url, and timedelta
  subsetData <- data %>% 
    filter(!!as.symbol(subsetVar)==1) %>% 
    select(-c(starts_with("data_channel_is_"), url, timedelta))
  
  return(list(subsetData, subsetVar))
}

subsettingDataReturn <- subsettingData(newsData, "lifestyle")
data <- subsettingDataReturn[[1]]
channel <- subsettingDataReturn[[2]]
```

# Exploratory Data Analysis 

## Column Description 
Let us take a look at the columns available. 

```{r}
colnames(data)
str(data)
```

Whew! That is a long list of columns to analyze. Instead of analyzing them all, let us think about our data. What might we expect to be related to how many times an article is shared? We hear frequently about how the dissemination of news and the content thereof are related in some way. For our data channels, let's pay close attention to the number of shares (dissemination) and variables that we might be able to link to it. Can we find any interesting relationships in the exploratory analysis? And do these relationships change across the different channels? Maybe the sharing of lifestyle articles is less correlated with sentiment than, say, world news articles.  


First, let's take a look at the variable descriptions for some better understanding. Here is a data description from the UCI Machine Learning Repository: 

- n_tokens_title: Number of words in the title
- n_tokens_content Number of words in the content
- n_unique_tokens: Rate of unique words in the content
- n_non_stop_unique_tokens: Rate of unique non-stop words in the content
- num_hrefs: Number of links
- num_self_hrefs: Number of links to other articles published by Mashable
- num_imgs: Number of images
- num_videos: Number of videos
- average_token_length: Average length of the words in the content
- num_keywords: Number of keywords in the metadata
- self_reference_min_shares: Min. shares of referenced articles in Mashable
- self_reference_max_shares: Max. shares of referenced articles in Mashable
- self_reference_avg_sharess: Avg. shares of referenced articles in Mashable
- global_subjectivity: Text subjectivity
- global_sentiment_polarity: Text sentiment polarity
- global_rate_positive_words: Rate of positive words in the content
- global_rate_negative_words: Rate of negative words in the content
- rate_positive_words: Rate of positive words among non-neutral tokens
- rate_negative_words: Rate of negative words among non-neutral tokens
- title_subjectivity: Title subjectivity
- title_sentiment_polarity: Title polarity
- abs_title_subjectivity: Absolute subjectivity level
- abs_title_sentiment_polarity: Absolute polarity level
- shares: Number of shares (target)


Below we run the five-number summary for each of the variables thus far still included. 

## Summary Statistics
```{r summarystats}
print(paste("******Summary Statistics of", channel, "******"))
summary(data)
```

## Target Variable Distribution 
Let's take a look at the distribution of our target variable using a histogram.

```{r}
ggplot(data) +
  aes(x = shares) +
  geom_histogram(bins = 26L, fill = "#112446") +
  labs(title = "Distribution of Shares") +
  theme_gray()
```

*What does the distribution show? Does the distribution follow a known distribution? Is there skewness? What might these features tell us about the number of shares? The number of shares is plotted along the x-axis, with frequency (count) on the y-axis.*

## Title Tokens vs Shares 
Now let's analyze the affect of the different variables on the shares. Starting with the number of words in the title and how they affect the shares. 

```{r}
data %>% 
  group_by(n_tokens_title) %>% 
  summarise(avgShares = mean(shares)) %>% 
  ggplot() +
  aes(x = avgShares, y = n_tokens_title) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Average Shares vs Title Tokens") +
  theme_gray()
```
*The average number of shares is plotted on the x-axis while the numvber of words in the article title is plotted on the y-axis. Can we see any relationship between the two variables?*

## Number of Links in the Articles vs Shares

```{r}
data %>% 
  group_by(num_hrefs) %>% 
  summarise(avgShares = mean(shares)) %>% 
  ggplot() +
  aes(x = avgShares, y = num_hrefs) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Average Shares vs Number of Links") +
  theme_gray()
```
*The average number of shares is plotted on the x-axis while the number of hyperlinks is plotted on the y-axis. Like with the previous plot, we use a scatter plot because we have two numeric variables, with the average number of shares being continuous. Can we see any relationship between the two variables?*

## Number of Images vs Shares

```{r}
data %>% 
  group_by(factor(num_imgs)) %>% 
  summarise(sumShares = sum(shares)) %>% 
  ggplot() +
  aes(x = `factor(num_imgs)`, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Shares vs Images", x = "Number of Images", y = "Shares(Sum)") +
  theme_minimal()
```
*The above bar plot demonstrates the relationship between the number of images in an article (x-axis) and the sum of the shares the article experienced. Can we see any patterns in the above visualization?*

## Number of Videos vs Shares

```{r}
data %>% 
  group_by(factor(num_videos)) %>% 
  summarise(sumShares = sum(shares)) %>% 
  ggplot() +
  aes(x = `factor(num_videos)`, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Shares vs Videos", x = "Number of Videos", y = "Shares(Sum)") +
  theme_minimal()
```

*In the above bar plot the number of videos featured in an article is plotted against the summed shares per video number. Do we notice any patterns? Can we make any comparisons between this plot (with videos) vs the previous plot, which looks at the number of images in an article?*

## Days of the Week and Shares

```{r}
mon <- data %>% 
  select(starts_with("weekday_is_monday"), shares) %>% 
  group_by(weekday_is_monday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_monday) %>% 
  filter(day==1) 
mon$day[mon$day==1] <- "MON"

tue <- data %>% 
  select(starts_with("weekday_is_tuesday"), shares) %>% 
  group_by(weekday_is_tuesday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_tuesday) %>% 
  filter(day==1)
tue$day[tue$day==1] <- "TUE"


wed <- data %>% 
  select(starts_with("weekday_is_wednesday"), shares) %>% 
  group_by(weekday_is_wednesday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_wednesday) %>% 
  filter(day==1)
wed$day[wed$day==1] <- "WED"


thu <- data %>% 
  select(starts_with("weekday_is_thursday"), shares) %>% 
  group_by(weekday_is_thursday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_thursday) %>% 
  filter(day==1)
thu$day[thu$day==1] <- "THU"

fri <- data %>% 
  select(starts_with("weekday_is_friday"), shares) %>% 
  group_by(weekday_is_friday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_friday) %>% 
  filter(day==1)
fri$day[fri$day==1] <- "FRI"

sat <- data %>% 
  select(starts_with("weekday_is_saturday"), shares) %>% 
  group_by(weekday_is_saturday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_saturday) %>% 
  filter(day==1)
sat$day[sat$day==1] <- "SAT"

sun <- data %>% 
  select(starts_with("weekday_is_sunday"), shares) %>% 
  group_by(weekday_is_sunday) %>% 
  summarise(sumShares = sum(shares)) %>% 
  rename(day = weekday_is_sunday) %>% 
  filter(day==1)
sun$day[sun$day==1] <- "SUN"

mon %>% 
  bind_rows(tue, wed, thu, fri, sat, sun) %>% 
  ggplot() +
  aes(x = day, y = sumShares) +
  geom_col(fill = "#112446") +
  labs(title = "Most Shared Articles by the Day of the Week") +
  theme_gray()
```


*The above bar plot looks at the sum of shares given for each day of the week. Are there any patterns? Are there differences in the number of shares between weekdays and the weekend? If so, what might cause this? Are articles also most likely to be published on certain days of the week, and thus more likely to be shared on those days? We can speculate.*

*In the three scatter plots below, we take a magnifying glass to some of the variables measuring features of article sentiment. We suspect there might be some patterns below (not guaranteed!). Can we use this as a starting point for investigating how article sentiment influences the dissemination of information (if at all)?*

## Title Polarity vs Shares

Polarity is a float which lies in the range of [-1,1] where 1 refers to a positive statement and -1 refers to a negative statement. Does title polarity affect the average number of shares?

```{r}
data %>% 
  ggplot() +
  aes(x = title_sentiment_polarity, y = shares) +
  geom_point() +
  geom_jitter() +
  labs(title = "Shares vs Title Polarity", x = "Title Polarity", y = "Number of Shares") +
  theme_minimal()
```

*The above scatter plot looks at title polarity (how negative or positive an article title might be) and the number of shares for a given article. Can we see any initial patterns worth exploring? *

## Global Polarity vs Shares

```{r}
data %>% 
  ggplot() +
  aes(x = global_sentiment_polarity, y = shares) +
  geom_point() +
  geom_jitter() +
  labs(title = "Shares vs Text Polarity", x = "Text Polarity", y = "Number of Shares") +
  theme_minimal()
```

*The above scatter plot is similar to the previous scatter plot, though this time we take a look at the text polarity (how positive or negative the words of the article are) and plot it against the number of times a given article is shared (y-axis). Again, do we notice any patterns?*

## Subjectivity and Shares

Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is a float which lies in the range of [0,1]. A value closer to 0 means an opinion or an emotion and 1 means a fact. How does the text having a factual tone or an author's emotion/opinion affect the total shares?

```{r}
ggplot(data) +
  aes(x = shares, y = global_subjectivity) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Shares vs Text Subjectivity", x = "Text Subjectivity", y = "Number of Shares") +
  theme_minimal()
```

*In the above scatter plot, we plot the text subjectivity against the number of times an article is shared (y-axis). Though subjectivity is not sentiment, we might have reason to suspect that they could be related--are subjective articles more mysterious, more enticing, more prone to "clickbait"? Does this scatter plot seem to convey anything like this?*

## How does the rate of negative words in an article affect the Shares?

```{r}
data %>% 
  group_by(rate_negative_words) %>% 
  summarise(avgShares = mean(shares)) %>% 
  ggplot() +
  aes(x = avgShares, y = rate_negative_words) +
  geom_point(shape = "circle", size = 1.5, colour = "#112446") +
  labs(title = "Average Shares vs Rate of Negative Words") +
  theme_gray()
```

*Here we see how the rate of the usage of negative words throughout the article tends to affect the shares. Does an article with higher number of negative words tend to have lesser shares?*

## Correlation Analysis

Now  that we have completed analysis of how the shares changes with the different variables, we do notice that there are way too many variables in this dataset. Feeding all these variables into the training models would mean "Garbage In and Garbage Out". One of the easiest ways to choose the variables to fit into the models is by checking the correlation. Potential predictors with high correlation between each other can prove problematic as they introduce multicollinearity into the model. We can remove some of this redundancy from the outset. While there are some models that thrive on correlated predictors. other models may benefit from reducing the level of correlation between the predictors.

Let us first understand the pair plots for all the variables explaining keywords.

```{r}
pairs(~ kw_min_min + kw_max_min + kw_min_max + kw_avg_max + kw_max_avg + kw_avg_min + kw_max_max + kw_min_avg + kw_avg_avg, data = data)
```

```{r}
cor(data[, c('kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_max_avg', 'kw_avg_min', 'kw_max_max', 'kw_min_avg', 'kw_avg_avg')])
```


```{r}
kwCorData <- as.data.frame(as.table(cor(data[, c('kw_min_min', 'kw_max_min', 'kw_min_max', 'kw_avg_max', 'kw_max_avg', 'kw_avg_min', 'kw_max_max', 'kw_min_avg', 'kw_avg_avg')])))

colRemove <- kwCorData %>% 
  filter(abs(Freq)>0.8 & Freq!=1 )

colRemove <- as.vector(colRemove$Var2)

data <- data %>% 
  select(-all_of(colRemove))
```

This removes all the highly correlated keyword variables that convey the same information. Now we will similarly investigate the self-referenced shares. 


```{r}
pairs(~ self_reference_avg_sharess + self_reference_max_shares + self_reference_min_shares, data = data)
```

If we find that any of the self_reference shares variables have a correlation of greater than 0.8 with one another, then we will eliminate it from the analysis. Again, this is done to limit the multicollinearity in the models we build below as well as reduce their dimension. We want to simplify our models from the outset as much as possible without losing predictors that will explain much of the variability in the number of times an article is shared. 

```{r}
srCorData <- as.data.frame(as.table(cor(data[, c('self_reference_avg_sharess', 'self_reference_max_shares', 'self_reference_min_shares')])))

colRemove <- srCorData %>% 
  filter(abs(Freq)>0.8 & Freq!=1)

colRemove <- as.vector(colRemove$Var2)

data <- data %>% 
  select(-all_of(colRemove))
```

In this next step, we examine our remaining variables to see if any share a correlation of 0.70 or higher. If so, we will remove it from the data. 

```{r}
descrCor <- cor(data) 
highlyCorVar <- findCorrelation(descrCor, cutoff = .85)
data <- data[,-highlyCorVar]
```

Again, we do not want to remove both the highly correlated variables. For example, if we were looking at the variables temperature in Farenheit and temperature in Celcius in predicting the number of people at a beach, both variable would be telling us the same thing, but we would still want to keep one of them because of its probable importance to the model. 
We will also remove `is_weekend` from our analysis as the variables `weekday_is_sunday` and `weekday_is_saturday` capture the same information. 

```{r}
data <- data %>% 
  select(-c("is_weekend"))
```

We are now down from `r ncol(newsData)` columns to `r ncol(data)` columns.

Let us finally do a correlation plot for all variables with threshold greater than 0.55 for the the present dataframe. 

```{r}
cols <- names(data)

corrDf <- data.frame(t(combn(cols,2)), stringsAsFactors = F) %>%
  rowwise() %>%
  mutate(v = cor(data[,X1], data[,X2]))

corrDf <- corrDf %>% 
  filter(abs(v)>0.55) %>% 
  arrange(desc(v))
corrDf

#turn corr back into matrix in order to plot with corrplot
correlationMat <- reshape2::acast(corrDf, X1~X2, value.var="v")
  
#plot correlations visually
corrplot(correlationMat, is.corr=FALSE, tl.col="black", na.label=" ")
```

Now that we have a more concise data set, let's zero in on the relationship between our target variable, shares, and the remaining variables. Below we extract the five variables that have the highest correlation with the shares variable. This may be a valuable insight prior to training our models.  

```{r}
sharesCor <- cor(data[ , colnames(data) != "shares"],  # Calculate correlations
                data$shares)

sharesCor <- data.frame(sharesCor)
sharesCor$names <- rownames(sharesCor)
rownames(sharesCor) <- NULL

sharesCor <- sharesCor %>% 
  rename(corrcoeff=sharesCor) %>% 
  arrange(desc(abs(corrcoeff))) %>% 
  head(6)

par(mfrow=c(2,3))
for (i in sharesCor$names) {
  plot(data$shares, data[[i]], ylab = i, main = paste("Shares vs", i))
}

#for (i in sharesCor$names) {
 # ggpairs(data[[i]], ylab = i, main = paste("Shares vs", i))
#}
```

# Data Splitting

Data splitting is an important aspect of data science, particularly for creating predictive models based on data. This technique helps ensure the creation of data models and processes that use data models -- such as machine learning -- are accurate. In a basic two-part data split, the training data set is used to train and fit models. Training sets are commonly used to estimate different parameters or to compare different model performance. The testing data set is used after the training is done; we see if our trained models are effective in predicting future values. We will use a 70-30 split on the dataset.

```{r}
train_index <- createDataPartition(data$shares, p = 0.7, 
                                   list = FALSE)
train <- data[train_index, ]
test <- data[-train_index, ]
```

We will check the shape of the train and test set

```{r}
print("The train set dimensions")
dim(train)
print("The test set dimensions")
dim(test)
```

# Modeling

We will be comparing linear and ensemble techniques for predicting shares. Each section below elucidates the model used and the reasoning behind it. 

## Linear Regression 

A simple linear regression refers to a linear equation that captures the relationship between a response variable, $Y$, and a predictor variable $X$.  The relationship is modeled below:  

$$Y = \beta_0 + \beta_1X_1 +\epsilon i$$

Where $\beta_0$ is the intercept and $\beta_1$ is the slope of the line. This relationship can be extended to the case in which the response variable is modeled as a function of more than one predictor variable. This is the case of a multiple linear regression, which is as follows:  

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + â€¦ + \beta_nX_n + \epsilon i$$

Where $\beta_0$ is the intercept and all $\beta$ are slope coefficients. For both simple and multiple linear regression cases, the Method of Least Squares is widely used in summarizing the data. The least squares method minimizes values of $\beta_0$ and all $\beta_n$, seen below:  

$$\sum_{i = 1}^{n} (yi - \beta_0 - \sum_{j = 1}^{k} \beta_j x_{ij}^2)$$

Since we are dealing with `r ncol(data)` variables, it is probably important to know that we would need to employ a feature selection/dimension reduction technique. Feature selection is the process of reducing the number of input variables when developing a predictive model.
It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in some cases, to improve the performance of the model. To prove this, we will first fit a full-model (with all the available variables) with multiple linear regression. 

```{r, warning=FALSE}
trControl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

mlrWithoutVS <- train(shares ~ .,
                     data = train,
                     preProcess = c("center", "scale"),
                     method = "lm", 
                     trControl = trControl)

summary(mlrWithoutVS)
```

```{r}
mlrWithoutVS$results
```

As we can see, the Root Mean Square Error of the model is `mlrWithoutVS$results$RMSE`. Now let us see how this changes with the Principle Components Analysis below.  

### Linear Regression with Dimensionality Reduction

Principle Components Analysis (PCA) is a dimension-reduction technique that can be extended to regression. In a PCA we find linear combinations of the predictor variables that account for most of the variability in the model. What this does is it reduces the number of variables $p$ into $m$ principal components, allowing for a reduction in complexity all the while retaining most of the variability of the $p$ variables. We extend this to regression by treating our $m$ principle components as predictors, though we cannot interpret them in the same way.  

```{r}
pcs <- prcomp(train, scale = TRUE, center = TRUE)
summary(pcs)
```

How many principle components should we use? This is somewhat subjective. Consider the plot below. How many principle components would be required in order to retain say 80 or 90 percent of the variability in the data? If we can effectively reduce the number of variables by way of this method, then we may want to consider a regression of these principle components, even if we lose some interpretability. 

```{r}
par(mfrow = c(1, 2))
plot(pcs$sdev^2/sum(pcs$sdev^2), xlab = "Principal Component", 
		 ylab = "Proportion of Variance Explained", ylim = c(0, 1), type = 'b')
plot(cumsum(pcs$sdev^2/sum(pcs$sdev^2)), xlab = "Principal Component", 
ylab = "Cum. Prop of Variance Explained", ylim = c(0, 1), type = 'b')
```

```{r}
pcaVar <- as.vector(cumsum(pcs$sdev^2/sum(pcs$sdev^2)))
for (i in seq_along(pcaVar)) {
  if(pcaVar[i] > 0.9 & pcaVar[i] < 0.92){
    pcaIndex = i
  }
}

pc_train <- predict(pcs, train)
pc_train <- data.frame(pc_train)
pc_train <- pc_train %>% 
  select(all_of(c(1:pcaIndex))) %>% 
  mutate(shares = train$shares)
pc_test <- data.frame(predict(pcs, test))  %>% 
  mutate(shares = test$shares)
```

We will now fit a multiple linear regression using these principle components. 

```{r}
mlrWitVS <- train(shares ~ .,
                  data = pc_train,
                  preProcess = c("center", "scale"),
                  method = "lm", 
                  trControl = trControl)

summary(mlrWitVS)
```

```{r}
mlrWitVS$results
```

Now that we have a Multiple Linear Regression with PCA, let us see how a Lasso Regression on the original dataset competes in terms of variable selection. 

```{r, warning=FALSE}
tuneGrid <- expand.grid(
  .fraction = seq(0, 1, by = 0.1)
)

lassoModel <- train(
  shares ~ .,
  data = train,
  method = 'lasso',
  preProcess = c("center", "scale"),
  trControl = trControl,
  tuneGrid = tuneGrid
)
```


```{r}
lassoModel$results
```

## Random Forest

The random forest model refers to an ensemble method of either classification or regression. In this case, we are predicting a continuous response variable, and are thus using the latter case. The random forest creates numerous trees from bootstrap samples of the data. Bootstrap samples are simply samples taken from the data and are of the same size (sample $n$ equals bootstrap $m$), meaning that an observation from the sample data could be used twice in the bootstrap sample, for example. A tree is fit to each bootstrap sample, and for each fit a random subset (generally $m = p/3$ predictors) of predictors is chosen. This is done with the tuning parameter mtry. 

Generally speaking, random forests predict more accurately because the results of the fitted trees are averaged across all trees. This averaging reduces variance. 

```{r}
tuneGrid = expand.grid(mtry = 1:3)

rfModel <- train(shares ~ .,
                  data = train,
                  method = "rf", 
                  trControl = trControl,
                  tuneGrid = tuneGrid)
```

Looking at the Variable Importance Plot : 

```{r}
plot(varImp(rfModel))
```

```{r}
rfModel$results
```

## Boosted Tree

Boosted tree models, like the random forest, are an ensemble tree-based method that can be used for classification or regression. Again, in our case, we are predicting a continuous response and are using regression. 

Unlike the random forest, in the boosted tree method, trees are grown sequentially, and for each tree the residuals are treated as the response. This is exactly true for the first tree. Updated predictions can be modeled by the following:  

$$\hat{y} = \hat{y}(x) + \lambda \hat{y}^b(x)$$

Below we fit our boosted tree model to the training data set.

```{r, results=FALSE}
tuneGrid = expand.grid(n.trees = seq(5, 30, 5), interaction.depth = seq(1,10,1), shrinkage = 0.1, n.minobsinnode = 20)

# fit the model
boostingModel <- train(shares ~ .,
                  data = train,
                  method = "gbm", 
                  trControl = trControl,
                  tuneGrid = tuneGrid,
                  verbose = FALSE)

```

# Model Comparison 

## Train Model Comparison

Now although using the accuracy on the testing data is the gold standard for model comparison, it can be imperative to check the train model accuracy to see how the models have fit the dataset. If there is a huge difference in train accuracies then we know that a certain model does not fit the data well. Here is a summarisation of the models with their hyperparameters and model metrics.

```{r}
trainModelComparison <- mlrWithoutVS$results[which.min(mlrWithoutVS$results$RMSE),] %>% 
  bind_rows(mlrWitVS$results[which.min(mlrWitVS$results$RMSE),],
            lassoModel$results[which.min(lassoModel$results$RMSE),], 
            rfModel$results[which.min(mlrWithoutVS$results$RMSE),],
            boostingModel$results[which.min(boostingModel$results$RMSE),]) %>% 
  mutate(Model = c("MLR", "MLR with PCA", "Lasso", "Random Forest", "Boosted Tree with PCA")) %>% 
  select(Model, everything())
trainModelComparison
```

We see that the model with the lowest RMSE value is `r trainModelComparison[which.min(trainModelComparison$RMSE),"Model"]` with an RMSE value of `r round(trainModelComparison[which.min(trainModelComparison$RMSE),"RMSE"],2)`. The model that performs the poorest `r trainModelComparison[which.max(trainModelComparison$RMSE),"Model"]` with an RMSE value of `r round(trainModelComparison[which.max(trainModelComparison$RMSE),"RMSE"],2)` which means that the model was incapable of fitting the data well.

## Test Model Comparison

Now that we have created our trained models (models fit to the training data) we should now see how accurately they predict future values. Once we have evaluated each of the models, we should be able to compare them to see which is best at making predictions on future data. We can do this by comparing the predicted values of the tested with the actual test set values. 

Withe function `postResample()` we can find our RMSE on the test data set and compare it across models. 

```{r, warning=FALSE}
predLinearTest <- predict(mlrWithoutVS, test)
testMLR<- postResample(pred = predLinearTest, obs = test$shares)
```

```{r}
predPCAtest <- predict(mlrWitVS, pc_test)
testPCA <- postResample(pred = predPCAtest, obs = pc_test$shares)
```

```{r}
predLassoTest <- predict(lassoModel, test)
testLasso <- postResample(pred = predLassoTest, test$shares)
```

```{r}
predRandomForest <- predict(rfModel, test)
testRandomForest <- postResample(pred = predRandomForest, obs = test$shares)
```

```{r}
predBoosting <- predict(boostingModel, test)
testBoosting <- postResample(pred = predBoosting, obs = test$shares)
```

In comparing the above models, we should be looking at which among the models best minimizes the RMSE, as the model with the lowest RMSE will, on average, make the most accurate predictions. 

```{r}
testModelComparison <- testMLR %>% 
  bind_rows(testPCA, testLasso, testRandomForest, testBoosting) %>% 
  mutate(Model = c("MLR", "MLR with PCA", "Lasso", "Random Forest", "Boosted Tree with PCA")) %>% 
  select(Model, everything())
testModelComparison
```

We see that the model with the lowest RMSE value is `r testModelComparison[which.min(testModelComparison$RMSE),"Model"]` with an RMSE value of `r round(testModelComparison[which.min(testModelComparison$RMSE),"RMSE"],2)`. The model that performs the poorest `r testModelComparison[which.max(testModelComparison$RMSE),"Model"]` with an RMSE value of `r round(testModelComparison[which.max(testModelComparison$RMSE),"RMSE"],2)`. 

# Conclusion 
Here's is general observation on the modelling with data with high dimension : When there are large number of features with less data-sets(with low noise), linear regressions may outperform Decision trees/random forests. There is no thumb rule on which model will perform the best on what kind of dataset. If you are dealing with a dataset with high dimensionality then your first approach must be to decrease this dimensionality before fitting the models. Both LASSO and PCA are dimensionality reduction techniques. Both methods can reduce the dimensionality of a dataset but follow different styles. LASSO, as a feature selection method, focuses on deletion of irrelevant or redundant features. PCA, as a dimension reduction method, combines the features into a smaller number of aggregated components (a.k.a., the new features).